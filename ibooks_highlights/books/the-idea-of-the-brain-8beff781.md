---
asset_id: 8BEFF781AAD43CE6B09DFA277E3C259E
author: Matthew Cobb
modified_date: '2022-02-06T22:29:32'
title: The Idea of the Brain
---

# The Idea of the Brain

By Matthew Cobb

## My notes <a name="my_notes_dont_delete"></a>



## iBooks notes <a name="ibooks_notes_dont_delete"></a>

The astronomer Lord Rees has pointed out that an insect is more complex than a star, while for Darwin the brain of an ant, which is so tiny but which can produce such diverse behaviour, was ‘one of the most marvellous atoms of matter in the world, perhaps more so than the brain of a man’. That is the scale of the challenge before us.

Among the writings of Galen that Haly Abbas translated were those covering the structure and the role of the brain: ‘The brain is the principal organ of the psychical members. For within the brain is seated memory, reason and intellect, and from the brain is distributed the power, sensation and voluntary motion.’

In 1543 two books were published that, on very different scales, helped transform how we look at the universe and its inhabitants. The first, De Revolutionibus Orbium Coelestium (On the Revolutions of the Heavenly Spheres) by Nicolaus Copernicus, outlined a mathematical model in which the earth rotated around the sun, using theorems developed over two centuries earlier by Arabic astronomers. The second was De Humani Corporis Fabrica (On the Fabric of the Human Body) by Andreas Vesalius. Running to over 700 pages in seven books, the Fabrica combined knowledge and aesthetics, presenting its readers with the most accurate description of human anatomy ever assembled. Vesalius used the power of the new printing technology to the full, enriching his text with more than 200 striking woodcut illustrations, all based on his dissections of human bodies.

The most notorious contribution to the eighteenth-century debate over thinking matter came from another of Boerhaave’s students, the Frenchman Julien Offray de La Mettrie. In 1747 La Mettrie published L’Homme machine (Machine Man), a manifesto for a new way of looking at the human mind and body, in which all the workings of the body and the mind could be explained by matter.58 As La Mettrie wrote: ‘all the soul’s faculties depend so much on the specific organisation of the brain and of the whole body that they are clearly nothing but that very organisation’.59 There was such a thing as thinking matter, claimed La Mettrie, and it was the brain.

Volta created what he described as an artificial electric organ based on the torpedo’s anatomy and composed of alternating discs of zinc and copper, interspersed with pieces of cardboard soaked in diluted acid. This was called a pile, after the pile of discs that it was made of–the term persists in French, but in English we now say ‘battery’.*
Amazingly, this device created a continuous current of electricity by the interaction of its components. Volta’s argument with Galvani had led to a new source of energy. This momentous finding was announced to the world in a letter to the Royal Society, written in March 1800 and published in June of the same year.31 The age of chemical electricity was born, and soon physicists and chemists all over Europe were using batteries in their research, spellbinding the public with demonstrations of the new form of power, such as at Humphry Davy’s famous lectures in London. In 1812 a teenage girl is thought to have attended one of Davy’s dramatic demonstrations of electricity; her name was Mary Godwin, but she would be better known by her married name, Shelley.32

The breakthrough came as a result of work inspired by one of the nineteenth century’s greatest scientists, Johannes Müller of the University of Berlin.55 Müller was particularly interested in the nature of nervous activity and its links with mind and perception–in his mid-twenties he noticed that if you stimulated a particular kind of nerve (say, the nerves in your retina by pressing your eyeball), then the stimulus was perceived not in terms of its physical nature (in this case, pressure), but in terms of the sense that the nerve normally communicated (vision). Müller called this effect ‘the law of specific nerve energies’: he imagined that each peripheral nerve carried a particular kind of energy, depending on the sensory organ it was connected to.
One of the reasons Müller adopted this position was that he did not accept that nerves transmitted electricity. Instead, he considered that organisms contained a vital principle that kept them alive and which was involved in the working of the mind and the production of behaviour. This vitalist view was typical of the Romantic movement in early nineteenth-century Europe and was one of the threads that contributed to Mary Shelley’s Frankenstein. For Müller, all talk of electricity in organisms was mere metaphor

These surprisingly slow speeds posed two problems. First, as Helmholtz realised, it had consequences for perception, as it would mean the brain could only respond to events in the past. Helmholtz dismissed this as a cause of any major problems in the real world: ‘Happily, the distances our sense-perceptions have to traverse before they reach the brain are short, otherwise our consciousness would always lag far behind the present.’62 Despite Helmholtz’s breezy confidence, the implication is indeed that we live–ever so slightly–in the past; we never perceive the world instantaneously.

Smee was a prolific writer, and a year later he produced a popular version of his theory in a book called Instinct and Reason. Some of his ideas appear remarkably prescient. Taking as his starting point the assumption that ‘light falling upon the nerve determines a voltaic current which passes through the nerves to the brain’, Smee suggested it would be possible to make an artificial eye, by aggregating ‘a number of tubes communicating with photo-voltaic circuits’. All you had to do was to repeat these structures over and over and then ‘there is no reason why a view of St Paul’s in London should not be carried to Edinburgh through tubes like the nerves which carry the impression to the brain’.68 Similar approaches were possible with the other senses, he argued. If sensation was electric, then it should be possible to build devices that could imitate it.

In 1842 Flourens wrote a book-length counterblast against phrenology, and particularly against Gall, who had been dead for fourteen years. Flourens justified his place at the heart of the French academic establishment by attacking Gall not only with experimental findings, but also by invoking the quintessential French philosopher of mind, Descartes, to whom the book was dedicated. Flourens’s experimental studies suggested that the mind was, as Descartes had argued from philosophical premises, a unitary whole. Flourens showed that many of the higher behavioural functions associated with mind and perception, in animals and in humans, appear not to be highly localised, as Gall had argued, but instead to be broadly distributed in the cortex. Localisation applied only either to basic physiological functions, or to those related to motor coordination. Evidence from strokes, for example, showed that if the right side of the brain was damaged, then the patient suffered from paralysis of some or all of the left side of the body. But this kind of contralateral motor impairment was relatively trivial compared with the deep mysteries of human mental existence. The mind, it appeared, was distributed across the cortex.

In his 1878 book The Localisation of Cerebral Function, Ferrier boldly drew parallels between Gage’s supposedly altered personality and his own observations of changes to the behaviour of monkeys with damaged frontal lobes. In many respects, the modern interpretation of Gage’s injuries and their significance can be traced back to Ferrier’s fusion of experimental, psychological and physiological insights–textbooks now frequently mention Gage, although they rarely get the complex story right.55

What made Huxley’s lecture so provocative was his claim that animals–and humans–were ‘conscious machines’ or ‘conscious automata’:
We are bound by everything we know of the operation of the nervous system to believe that when a certain molecular change is brought about in the central part of the nervous system, that change, in some way utterly unknown to us, causes that state of consciousness that we term a sensation.23

Huxley had form in this respect. In 1870 he had given a lecture on Descartes in which he explored the possibility that machines might be conscious:
I hold, with the Materialist, that the human body, like all living bodies, is a machine, all the operations of which will, sooner or later, be explained on physical principles. I believe that we shall, sooner or later, arrive at a mechanical equivalent of consciousness, just as we have arrived at a mechanical equivalent of heat.

It was explored by Hermann von Helmholtz in his Handbook of Physiological Optics, which appeared in 1867. For centuries, many of the philosophical discussions about the mind had focused on what happens when we perceive an object. A common-sense explanation was that perception is merely the consequence of the physical stimulation of the sense organs–we see what is in front of us, as though through a window. But Helmholtz realised that things are not so simple. In reality the nervous system, and in particular the brain, plays a highly active part in constructing our perception of even quite straightforward things. The brain does not simply register the outside world, it selects and represents aspects of it.

Helmholtz’s starting point was the existence of illusions such as when you press your eyeballs and perceive coloured patterns, or the distressing ‘phantom limb’ illusion when an amputee can still feel the limb even though it is no longer present. Helmholtz argued that these effects, which had led Müller to believe that each nerve had its own kind of energy, were in fact ‘an illusion in the judgement of the material presented to the senses, resulting in a false idea of it’. Helmholtz understood that in such cases stimulation of the nerves was being perceived either as though the usual sensory modality was involved (in the case of pressure on the eyeballs) or as though the absent limb was in fact present.

Helmholtz applied this insight to normal perception and argued that when we perceive, the nervous system draws what he called ‘unconscious conclusions’ about the nature of what is being perceived. Perceptions are not simple impressions produced by the environment, but rather ‘inductive conclusions, unconsciously formed’, he declared.31 Helmholtz’s explanation hinted at some kind of process in the nervous system that would be able to draw conclusions without the mind being aware of it. With sufficient repetition, the process became completely unconscious, he argued. We learn to perceive.

Helmholtz’s view of the brain as an active organ, and of perception as an imperfect and selective process leading to a view of the world, represented a major breakthrough in our understanding of what the brain does, one that still dominates today. This insight was the pure product of scientific discovery, without the application of a metaphor from technology. On the other hand, in one respect, philosophy had got there first. The eighteenth-century philosophers of perception such as Hume and Kant had argued over whether ideas came from the world (Hume), or whether we use innate concepts in our perception (Kant).

Sherrington provided a precise account of reflexes in dogs, in particular the scratch reflex, where stimulation of skin on the animal’s flank leads to a rhythmic scratching movement in the leg (try this by scratching the side of a friendly dog–it does not work so well on a cat). Sherrington showed how each sensory nerve was linked to a particular area of skin, which he called a receptive field, that if stimulated could induce the nerve to respond. Activation of any of these nerves led to the same behaviour–scratching–a muscular response that Sherrington called the ‘final common path’ of the scratch reflex arcs.41

### 7. Neurons

In the mid-1880s, Wilhelm His of the University of Leipzig reported he could see no fusion between nerve cells and concluded that they were indeed each an independent structure, like other cells. His also coined a new term to describe the complex tree-like part of a nerve cell–he called them dendrites after the Greek for tree (dendron). At around the same time a Swiss scientist, August Forel, cut the nerve fibre leading to the tongue and then, after a few days, studied which tissues in the brain died because they had been separated from the main part of the cells which supplied them with nourishment. To Forel’s surprise, only a tiny area of the brain was affected, indicating that nerve cells were not interconnected. The very specific and limited degeneration observed by Forel suggested that each cell body and its dendrites formed a single unit.

At the same time as Cajal and van Gehuchten were developing their ideas of unidirectional function at the microscopic level, the early psychologist William James generalised the conclusions of the gross anatomical and functional studies of nerves and muscles and the pathways they followed to create reflex arcs. As he put it in his 1890 book The Principles of Psychology:
paths all run one way, that is from ‘sensory’ cells into ‘motor’ cells, and from motor cells into muscles, without ever taking the reverse direction. A motor cell, for example, never awakens a sensory cell directly, but only through the incoming current caused by the bodily movements to which its discharge gives rise. And a sensory cell always discharges or normally tends to discharge towards the motor region. Let this direction be called the ‘forward’ direction. I call the law an hypothesis, but really it is an indubitable truth.22
To underline his point, James accompanied his argument with a number of diagrams showing the organisation of different cell types. In these figures the cells were all connected, as if in a network, and, like Cajal–but a year earlier–arrows were used to denote the direction of the hypothetical nerve currents.

Learning, Cajal claimed, led to increased connectivity and revealed what the Belgian scientist Jean Demoor called the plasticity of cerebral neurons.26 Cajal realised that this plasticity meant that only limited understanding could be gained by seeing the brain as a kind of telegraph system:
A continuous pre-established network–a kind of grid composed of telegraph wires in which neither new nodes nor new lines can be created–is something rigid, immutable, incapable of being changed, which clashes with the widespread impression that the organ of thought is, within certain limits, malleable and capable of perfection, above all during its development, by means of well-directed mental exercise.
Unable to point to any more sophisticated technological metaphor, Cajal retreated into describing the brain in terms of other forms of living matter:
At the risk of making a far-fetched comparison, I would defend this idea by saying that the cerebral cortex is like a garden full of an infinite number of trees–pyramidal cells–which, by careful cultivation, can produce more branches, push their roots deeper, and produce ever more varied and exquisite flowers and fruits.

Furthermore, although the idea that the brain routes messages to an appropriate destination is a powerful one, if taken literally it meant that each cell was only connected to one other cell, and that neuronal transmission was linear. Neuroanatomy showed that this was completely naive.

Sometimes, a problem has to be named before it can be fully understood. In this case, the breakthrough in understanding the transmission of nerve impulses began with naming the place where two neurons meet. In 1897 Sherrington was asked to contribute to a new edition of the Handbook of Physiology, edited by Michael Foster, Professor of Physiology at Cambridge. In his chapter, Sherrington introduced a term to describe how two cells interact:
So far as our present knowledge goes we are led to think that the tip of a twig of the arborescence is not continuous with but merely in contact with the substance of the dendrite or cell-body on which it impinges. Such a special connection of one nerve-cell with another might be called a synapsis.34
‘Synapsis’ was taken from the Greek for ‘clasp’, because it seemed that the arborescence of the axon of the incoming cell clasped the dendrites of the next cell. Within two years, synapsis, which was already in use in cell biology, had become synapse, the term we use today.

In his ground-breaking experimental work, Adrian had been able to deconstruct the nerve impulse and had revealed that it was composed of exceptionally brief pulses. Each of these pulses had the same shape, and yet despite this lack of variability, nerve activity was able to carry a message. To explain this, Adrian made an analogy that now seems obvious, but at the time was utterly novel:
The message consists merely of a series of brief impulses or waves of activity following one another more or less closely. In any one fibre the waves are all of the same form and the message can only be varied by changes in the frequency and duration of the discharge. In fact the sensory messages are scarcely more complex than a succession of dots in the Morse Code

Nowadays, the idea that organic structures such as genes or neurons contain some kind of code is relatively banal. Schoolchildren learn about the genetic code, while neuroscience students explore different forms of neural codes. But when Adrian was writing in the early 1930s, this was a completely new way of thinking about what neurons might do, and how brains might function. Furthermore, it pointed the way to a whole new area of research: if the message contained a code, then it should be possible to break that code, to reveal what neurons were telling the brain

### 9. Control

McCulloch had been thinking about this approach to biology for over fifteen years.17 His key insight came when he realised that the all-or-none nature of an action potential was the equivalent of a proposition in logic–a statement that is either true or false. The neuron either fires or it does not. This was an example of what McCulloch called a ‘psychon’–a basic mental ‘atom’, which combined with others to form more complex phenomena. He now understood that it should be possible to describe the activity of a series of neurons–what he called a ‘nervous net’–in terms of a series of propositions.

Their paper showed that nervous systems could be thought of in a highly abstract way; this insight seemed more powerful than any of the physical models that had been proposed in the previous decades. It represented a major departure from the dominant approach in understanding how the brain works, which for over half a century had been based around localising functions in the cortex, but which had done little more than identify vague ‘centres’ that were involved in various motor functions

The real novelty of McCulloch and Pitts’s work was that it focused attention on processes rather than on anatomical regions. Explaining the brain now appeared to involve describing algorithms that could be embodied in networks of neurons, or in interactions between organs. The key issue was the relation between the component parts 

and the way that function emerged from organisation–what McCulloch and Pitts called the immanent logic of neuronal structures.

Von Neumann also explained that brains were far smaller than any computer and contained far more components (this was before the days of transistors, which had become a practical proposition just a year earlier, allowing for a first step in miniaturisation, but the point is still valid today). Above all, he put his finger on what remains one of the major questions in neuroscience, using a verb that is now commonplace, but which at the time was novel: ‘how does [the neuron] encode a continuous number into a digital notation?’

The initial version of this robot, built of seventy-five bulky electromagnetic telephone relays, was a large grid with a finger sensor that moved over the surface of the maze; this was later upgraded to a more crowd-pleasing ‘mouse’, which was moved by magnets. The mouse–called Theseus–was the subject of a brief film in which Shannon claimed that its maze solving ‘involves a certain level of mental activity, something akin, perhaps, to a brain’.54 The robot impressed everyone, from the participants at the Cybernetics meeting (‘It is all too human,’ said one, uncritically55), through the readers of Time, Life and Popular Science magazines, to Shannon’s employers, who discussed making him a member of the board in recognition of his achievement.56 Despite the excitement, Theseus was simply a more sophisticated version of the mechanical maze robot built by Ross and Smith in the 1930s, and it, too, provided no insights into learning at all

Ryle’s The Concept of Mind did much to cement the conviction among many readers that the mind has a material basis. Ryle’s readable argument makes no attempt to explain either how the brain works, nor how the activity of the brain results in the existence of the mind–the word ‘brain’ is barely mentioned. Ryle’s main objective was to systematically dismantle Descartes’s dualism, which he disparagingly characterised as ‘the ghost in the machine’. His argument created a coherent philosophical basis for considering that mental life is identical with the physical activity of the brain, but it did not prove this to be the case.

From the 1930s onwards, the Montreal neurosurgeon Wilder Penfield carried out hundreds of brain operations with the aim of relieving chronic, debilitating temporal-lobe epilepsy.1 To identify which part of the brain to remove, Penfield gently stimulated his conscious patients with electric currents carried by delicate electrodes. If stimulation of a particular brain location led to indications that a seizure was imminent, Penfield knew that site was a candidate for removal. This procedure revealed something rather eerie: sometimes the stimulation led the patient to relive very precise events. These experiences were vivid and detailed, like a waking dream. Often the patients heard sounds–a piano being played, someone singing a well-known song, or a telephone conversation between two family members. In one case, when the electrode was left in place and the current flowed, the music continued in the patient’s head, and she sang along. In another, each time a particular region was stimulated, the patient heard an orchestra playing a popular song of the time, ‘Marching Along Together’. In yet other examples a patient saw a man and a dog walking along a road near his home, another saw jumbles of lights and colours, while yet another relived the recent experience of his mother telling his young brother he had his coat on backwards.

As Penfield put it ‘recollections which are clearly derived from a patient’s past memory can sometimes be forced upon him by the stimulating electrode’.2 These oneiric* experiences were remarkably constant for a given individual–repeated stimulation at the same location evoked exactly the same sensation in the patient. For Penfield these data suggested that memory might have a very precise location in the brain.

In the middle of the twentieth century, one of the dominant views in studies of the neural bases of memory was expressed by Karl Lashley, whose experiments on animals seemed to show that deficits in learning produced by surgical operations were proportional to the extent of the damage to the cortex. He explained this effect in two ways: first, all cells had equal capabilities; second, the whole of the brain contributed to the making and recollection of memories–what he called ‘mass action’. For Lashley, like Flourens in the nineteenth century, the activity of the brain could be understood only as a whole.

At the peak of his influence (he fell ill in 1954 and died four years later, aged sixty-eight), Lashley argued that memory was distributed across the brain. Reviewing his lifelong search for the engram, he suggested that it had been in vain and concluded wryly:

Figure by Penfield showing a vertical slice through the brain indicating where stimulation produced the perception of a song.

This series of experiments gives a good bit of information about what and where memory is not. It has discovered nothing directly of the real nature of the engram.

Lashley’s distributed view of memory was soon apparently contradicted by Penfield’s eerie findings, which he first reported at a meeting in 1951. Lashley was in the audience. Penfield explained the weird experiences of his patients in the following terms: as we pay conscious attention to events in our lives, we are ‘simultaneously recording it in the temporal cortex’.5 This record contained both visual and auditory stimuli, and was stored somewhere in a region below the cortex, in the middle of the brain, which is connected to the cortex by a complex set of nerve fibres. During stimulation, the impulses represented by these sensations ‘pass in the reverse direction to those which created that pattern’. To put it another way, the experience is played back through the same networks that recorded it. It would appear that Penfield had activated an engram.

In the discussion of Penfield’s presentation, Lashley had to admit he was stumped: ‘I have no clear alternative to offer in explanation of Dr Penfield’s data,’ he said. Nevertheless, he did his best to undermine Penfield’s observations by emphasising the complexity of memories before concluding, somewhat lamely, ‘I do not see how it is possible for the small number of cells in the centrencephalic system to mediate, or even transmit, these complexities

In 1937 Penfield had published some simpler results of stimulation of patients undergoing brain surgery, which at heart were like those of Fritsch and Hitzig and of Ferrier, but on conscious humans.10 Sometimes the patients would report very specific sensations when particular parts of their brain were stimulated–tingling in the fingers, odd tastes on the tongue, feelings of warmth down one side of the body. On other occasions eyelids flickered, legs jerked, and some patients grunted. To summarise their findings, Penfield asked a medical illustrator, Hortense Cantlie, to make a drawing.11 The result was a grotesque image of the human body, showing the various parts in proportion to their representation in the brains of their patients. This image, which Penfield called a homunculus, suggested how the brain sees the body. As might be expected on the basis of every day experience, the tongue, hands and face were particularly well represented. Other very sensitive parts of the body, such as the genitalia and the rectum, were not shown.
In 1950 Penfield presented a more sophisticated diagram, separating out the sensory regions (on the left) and the motor regions (on the right) of the brain, which was shown in cross-section

In 1949 Hebb published The Organisation of Behaviour, which set out key elements of what became the modern biological framework for understanding how the brain functions.14 Hebb’s starting point was straightforwardly materialist–the mind was simply a product of the activity of the brain.

One of his insights was a conception of how learning occurs at a cellular level. Opposing his one-time teacher Lashley, Hebb insisted ‘memory must be structural’.16 According to Hebb, that structure involved two levels–a complex ‘tridimensional lattice-like assembly of cells’ (less poetically, a network), and the way those cells were connected. What Hebb described as his neurophysiological postulate of memory was as follows: ‘When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased.’

Hebb argued that the complexity of many cell assemblies meant that ‘at each synapse there must be a considerable dispersion in the time of arrival of impulses, and in each individual fibre a constant variation of responsiveness’. This meant that the same assembly can function in a different way in different circumstances, and that distinct patterns of activation corresponding to distinct stimuli or memories therefore occur not only in space, but also in time. The lattice-like assembly of cells that constituted Hebb’s version of the engram was four-dimensional.

Finally, despite his focus on learning, Hebb dismissed the widely held view that learned and instinctive behaviours were fundamentally different, asserting that ‘Ultimately, our aim must be to find out how the same fundamental neural principles determine all behaviour.’ For some researchers that is still the goal; others consider it to be a pipedream, because no such principles exist.

### 10. Memory

In 1971 O’Keefe, together with his student Jonathan Dostrovsky, reported data from eight cells in the hippocampus that were each activated when the rat was in specific locations in the cage. But it was not only location that was significant: the strongest response came from a cell that fired when the rat was in a particular place, was being held by an experimenter, and the lights were on. If any one of those factors was absent, the cell stopped firing, indicating that it required a very specific set of stimuli to be activated.

O’Keefe’s research revealed that as well as the ability to encode episodic memories, the hippocampus contains a literal map of the environment–the representation in the brain is isomorphic to the environment, to use the jargon. This map, consisting of what are called place cells, also contains information about how to get from one location to another, enabling the animal to navigate the world and to predict what it will find in different places. It is a map, but as Tolman had brilliantly intuited, it is a cognitive map, involving multiple sensory modalities and based on associations and predictions, not a simple one-to-one representation of the outside world. In species with different ecologies these hippocampal maps have different forms–for example, while the maps are 2-D in rats they are 3-D in bats, describing the animal’s position within a sphere–but they are always cognitive, not simply spatial

Not many researchers would put it so robustly, but Eichenbaum’s argument highlights the fact that the memories that are processed by the hippocampus involve distant regions of the brain. The hippocampus is not the site of the engram, it is the encoder and the gateway. There is localisation of memory but we have yet to identify exactly where it is found; distributed information is also involved, and we do not fully understand how the encoding and recall of memory take place in the hippocampus and its associated areas

Hubel and Wiesel’s work suggested that visual processing in the brain is organised along some kind of hierarchical structure within which objects are identified with increasing levels of precision by increasingly localised structures as the hierarchy is ascended.

The existence of higher-level dedicated circuits in vision was highlighted in 1992, when David Milner and Melvyn Goodale suggested that there are two separate visual processing streams in the mammalian brain, with different output functions.20 After initial processing in the visual cortex at the back of the brain, visual information is split into two pathways, one of which goes to the top of the brain–this is called the ‘where’ pathway or dorsal stream, and is thought to encode information about the spatial location of the object that has been detected, projecting into regions involved in motor control. The other pathway, which goes deeper into the base of the cortex, is called the ventral stream, and is sometimes called the ‘what’ pathway. It is involved with identifying the objects that have been seen; this pathway projects to brain regions associated with memory and social behaviour. There are connections between the two streams–at some point, looking at a cat and wanting to stroke it, you need to put those two things together.
The distinctions between the two streams–where and what, dorsal and ventral, identity and action–underline the complexity of functional localisation in the brain.21 The function that is localised is not just a physical aspect of the stimulus, but also some aspects of a stimulus that require the organism to be ready to respond in a particular way–reaching out for it, or remembering it.22 This is a much less rigid way of thinking about functional localisation than imagining that all aspects of our grandmother are located in the same area.23 But as the number of interconnections grows, and the involvement of different sensory modalities in similar neural tracts is discovered, the idea that function is fully localised is gradually becoming weaker. Our understanding of what exactly is localised is becoming more confused–or richer, if you prefer.

Barlow’s starting point–his first dogma–was that a full description of the working of the nervous system required a description not only of a cell’s activity but also of its role as a node in a network. The underlying principle of how such networks functioned, argued Barlow, was that ‘at progressively higher levels in sensory pathways information about the physical stimulus is carried by progressively fewer active neurons’. To explain this, Barlow referred to an idea put forward in 1890 by William James, who argued that the brain must contain a ‘pontifical cell’ which, like the Pope, presided over all other brain cells.27 Although there is no such anatomical structure, the term ‘pontifical cell’ stuck as a way of describing a highly hierarchical theory for the organisation of the brain. Barlow suggested that although there was no ‘pontifical’ cell, there might be what he jokingly called ‘cardinal cells’.

Overall, Barlow’s dogmas have fared well in the intervening years. In particular, his idea of cardinal cells has been recast in terms of what is called ‘sparse coding’, whereby the higher the level of representation, the fewer the number of cells involved and the sparser their activity, but the more significant it is in terms of both the overall activity of the system and the representation of the stimulus.

In reality, there are neurons that link whole areas, and sometimes the whole brain–a recent imaging study of five neurons in the mouse showed that they threaded their way round the brain with such complexity that their total length was over 30 cm.

Marr’s approach to understanding how a particular function is executed in the brain (or a computer) involved dividing the problem into three parts. First, the problem to be solved has to be stated logically; this theoretical approach frames how the problem is explored experimentally or is modelled. Second, the way the input and the output of the system are represented has to be determined, along with a description of an algorithm that could get the system from one state to another. Finally, it has to be explained how the second level could be implemented physically–in the case of brain activity, in the nervous system. Marr’s assumption was that the constraints on producing a network that can see, be it in a machine or in a brain, would be basically similar in all cases and would therefore lead to similar algorithms being used, even if there would be major differences in the way those algorithms were implemented in flesh or in silicon. By resolving the problem of vision in a machine, he argued, we would get a better grip on vision in our heads.

As Marr put it at a meeting in Cold Spring Harbor in 1976, ‘This contour is not being detected, it is being constructed.’21 This view–which can be traced back to Helmholtz–emphasises that the brain is not simply a passive observer receiving sensory information. Perception involves assembling and interpreting those stimuli. This approach is essential for any model of vision, for nothing will happen if the machine (or the retina) merely identifies degrees of light and shade at each point of an image. That is what a camera does, and cameras cannot see.

Tsao, whose brief Twitter bio reads ‘cortical geometer’, suspects that the kind of feature-extraction she has been able to reveal with regard to face detection may be a general process occurring across the visual cortex–‘We think the entire inferior temporal cortex may be using the same organisation into networks of connected patches, and the same code for all types of object recognition.’26 Her current challenge is to try and understand the neuronal basis of visual illusions, such as the well-known vase/face illusion; as she points out, a decade ago no one would have known where to start. Now we do.

### 14. Localisation

At the 1958 symposium where Selfridge unveiled his Pandemonium program, Gregory argued that identifying function by ablating or lesioning a particular structure was not only logically flawed but also failed to provide real insight–you might be focusing on the output of a damaged, misfiring system. To properly understand the role of the component, you need a theoretical model of how the system works. And therein lies the difficulty, argued Gregory: ‘The biologist has no “Maker’s Manuals” or any clear idea of what many of the “devices” he studies may be. He must guess the purpose, and put up for testing likely looking hypotheses for how it may function.

Over the years Gregory extended this critique, presenting all sorts of analogies to undermine researchers’ confidence that if a particular structure was ablated, the altered behaviour had to be localised to that part of the brain. These analogies often involved what at the time was cutting-edge technology, but which now looks quaint or even mysterious to young people–valves in television sets, spark plugs in car engines, and so on–but they all focused on the same problem of interpreting what appear to be 

simple experiments that remove a key component

In his sprawling 1981 masterwork, Mind in Science, Gregory questioned whether this was actually true, pointing out that particular functions could rarely be revealed by removing parts one by one:
One finds, rather, that bizarre things happen when parts are removed; or nothing may happen, except under special conditions such as extreme demands or loading. For example, spokes of a bicycle wheel can be removed, one by one with little effect until there is a sudden collapse. Removing parts from an electrical circuit may produce output characteristics that were not present–such as whistles for a radio or complex patterns for a television… In truth the relations between parts, and their causal interactions, and the functions that they achieve, are highly complex and subtle beyond common understanding. It is particularly difficult to say where functions are located. This is a most serious problem for brain research

Sensory stimuli are initially processed separately–there is no evidence of olfactory signals being represented in primate visual cortex V1, nor are visual signals found in the vertebrate olfactory bulb. This also applies to the equivalent structures in an insect brain. However, in mice, Terry Sejnowski’s group has shown that top-down inputs from the hippocampus 

and the entorhinal cortex project to the area of

the olfactory bulb where odours are identified.48 This implies that memory or stress may influence how we perceive odours; similar pathways may be found relating to vision, suggesting that the function of regions of the brain that process sensory stimuli may be more complex than was originally thought. And just a few synapses away from these areas, in the higher regions of the brain where, presumably, thinking is done in both you and an ant, things get interesting. The signals from the different sensory modalities are integrated and our understanding of what is localised becomes messy.

Crick seized on the idea that attention could be seen as a spotlight–the brain would serially focus on different elements of a visual scene, a function that should be detectable in the activity of neurons. Crick proposed that the searchlight is controlled by the reticular complex of the thalamus and that it should be possible to detect its attention-related activity in this area. The key point is not whether Crick was right or wrong (he was wrong), but that he was developing a way of approaching consciousness–choosing a clearly defined aspect and then looking for its neuronal basis. If something relatively simple could be understood, there was hope that the larger problems could eventually be dealt with, too

### 15. Consciousness

A radical challenge to our everyday experience of consciousness appeared in a series of studies by the veteran neuroscientist Benjamin Libet, contributing to the philosophical excitement that began in the 1980s and 1990s.64 Libet’s work is generally taken to undermine the notion of free will–our feeling that we can choose how to behave. In a very complicated experiment that has since been replicated many times in various forms, Libet found that EEG traces which revealed subjects’ intentions to move a finger slightly preceded their conscious decision to do so. For many scientists and some philosophers, this finding suggested that consciousness and free will, in the form of a mental homunculus, are an illusion. The conscious sensation of deciding to move your finger, they claim, is a rationalisation of a decision that has already been taken by your nervous system.

### FUTURE

This question was raised back in 1990 by Walter Freeman and Christine Skarda, when they published an article entitled ‘Representations: Who Needs Them?’13 Freeman, who had studied electrophysiological responses to odours for decades, explained that by ceasing to worry about how nervous systems represent the environment, he was able ‘to focus less on the outside world that is being put into the brain and more on what brains are doing’. The idea that nervous systems represent or encode information contains an even more fundamental implication. As Dennett said to Crick and Koch–represent to whom?

According to Buzsáki, the brain is not simply passively absorbing stimuli and representing them through a neural code, it is actively searching through alternative possibilities to test various options. His conclusion, building on the insights of Helmholtz and Marr, is that the brain does not represent information: it constructs it.